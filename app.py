import streamlit as st
from streamlit_webrtc import webrtc_streamer, VideoTransformerBase
import cv2
import pytesseract
import openai
from feedback_logic import generate_feedback


# OpenAI API ì„¤ì •
openai.api_key = st.secrets["OPENAI_API_KEY"]

# Streamlit ê¸°ë³¸ êµ¬ì„±
st.set_page_config(page_title="Math Feedback Service", layout="wide")
st.title("ì´ˆë“±í•™êµ ìˆ˜í•™ ë¬¸ì œ í”¼ë“œë°± ì„œë¹„ìŠ¤")

# ìƒíƒœ ì´ˆê¸°í™”
if "additional_info_visible" not in st.session_state:
    st.session_state["additional_info_visible"] = False
if "additional_info_content" not in st.session_state:
    st.session_state["additional_info_content"] = ""
if "camera_active" not in st.session_state:
    st.session_state["camera_active"] = False

# ë¬¸ì œ ì…ë ¥ ì˜ì—­
problem = st.text_area("ìˆ˜í•™ ë¬¸ì œë¥¼ ì…ë ¥í•˜ì„¸ìš”:", placeholder="ì˜ˆ: ì§ê°ì‚¼ê°í˜• ëª¨ì–‘ì˜ ì¢…ì´ë¥¼ ëŒë ¤ ì›ë¿”ì„ ë§Œë“¤ì—ˆì„ ë•Œ...")

# í”¼ë“œë°± ìš”ì²­ ë²„íŠ¼
if st.button("í”¼ë“œë°± ìƒì„±"):
    if problem.strip():
        with st.spinner("í”¼ë“œë°±ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
            feedback, requires_more_info = generate_feedback(problem)

            if requires_more_info:
                st.warning("ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ì¶”ê°€ ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤. ì•„ë˜ì— ì¶”ê°€ ì •ë³´ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
                st.session_state["additional_info_visible"] = True
            else:
                st.subheader("ìƒì„±ëœ í”¼ë“œë°±:")
                st.markdown(feedback)
    else:
        st.warning("ë¬¸ì œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”!")

# ì¶”ê°€ ì •ë³´ ì…ë ¥ ì˜ì—­ (ìƒíƒœ ê¸°ë°˜ìœ¼ë¡œ í‘œì‹œ)
if st.session_state["additional_info_visible"]:
    additional_info = st.text_area(
        "ë¬¸ì œì— ëŒ€í•œ ì¶”ê°€ ì •ë³´ë¥¼ ì…ë ¥í•˜ì„¸ìš”:",
        value=st.session_state["additional_info_content"],  # ìœ ì§€ëœ ê°’ì„ ì‚¬ìš©
        key="additional_info_input"
    )

    # ìƒíƒœ ì—…ë°ì´íŠ¸ ë²„íŠ¼ í´ë¦­ ì‹œ ì €ì¥
    if st.button("ì¶”ê°€ ì •ë³´ë¥¼ í¬í•¨í•œ í”¼ë“œë°± ìƒì„±"):
        st.session_state["additional_info_content"] = additional_info  # ì…ë ¥ëœ ê°’ì„ ìƒíƒœì— ì €ì¥
        combined_input = f"ë¬¸ì œ: {problem}\nì¶”ê°€ ì •ë³´: {st.session_state['additional_info_content']}"
        with st.spinner("í”¼ë“œë°±ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
            feedback, _ = generate_feedback(combined_input)
            st.subheader("ìƒì„±ëœ í”¼ë“œë°±:")
            st.markdown(feedback)

# OCR ê¸°ëŠ¥ ì¶”ê°€
class VideoTransformer(VideoTransformerBase):
    def __init__(self):
        self.frame = None

    def transform(self, frame):
        self.frame = frame.to_ndarray(format="bgr24")
        return self.frame

def extract_text_from_image(image):
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    text = pytesseract.image_to_string(gray)
    return text

st.header("OCR ê¸°ëŠ¥")

if st.session_state["camera_active"]:
    webrtc_ctx = webrtc_streamer(key="example", video_transformer_factory=VideoTransformer)
    if webrtc_ctx.video_transformer:
        if st.button("ğŸ“¸ ì´¬ì˜"):
            image = webrtc_ctx.video_transformer.frame
            if image is not None:
                st.image(image, caption="Captured Image", use_column_width=True)
                text = extract_text_from_image(image)
                st.subheader("Extracted Text")
                st.session_state["additional_info_content"] = text
                st.write(text)
                st.session_state["camera_active"] = False
            else:
                st.warning("No frame captured")
else:
    if st.button("ğŸ“· Start Camera"):
        st.session_state["camera_active"] = True
